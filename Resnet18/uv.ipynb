{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.metrics import precision_recall_curve, accuracy_score, precision_score, recall_score, accuracy_score,f1_score, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, csv_file, white_dir, uv365_dir, uv395_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to CSV with columns: [filename, label]\n",
    "            white_dir (str): Directory for WhiteNoUV images\n",
    "            uv365_dir (str): Directory for 365UV images\n",
    "            uv395_dir (str): Directory for 395UV images\n",
    "            transform (callable, optional): Transformations to apply to images\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.white_dir = white_dir\n",
    "        self.uv365_dir = uv365_dir\n",
    "        self.uv395_dir = uv395_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx]['filename']\n",
    "        img_name = str(int(img_name))+'.jpg'\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # Load images from all three modalities\n",
    "        white_img = Image.open(os.path.join(self.white_dir, img_name)).convert('RGB')\n",
    "        uv365_img = Image.open(os.path.join(self.uv365_dir, img_name)).convert('RGB')\n",
    "        uv395_img = Image.open(os.path.join(self.uv395_dir, img_name)).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            white_img = self.transform(white_img)\n",
    "            uv365_img = self.transform(uv365_img)\n",
    "            uv395_img = self.transform(uv395_img)\n",
    "\n",
    "        return (white_img, uv365_img, uv395_img), torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModelWithConvFusion(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(MultimodalModelWithConvFusion, self).__init__()\n",
    "        \n",
    "        # Backbones for each modality\n",
    "        self.white_backbone = models.resnet18(pretrained=True)\n",
    "        self.uv365_backbone = models.resnet18(pretrained=True)\n",
    "        self.uv395_backbone = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Remove final FC layers\n",
    "        self.white_backbone.fc = nn.Identity()\n",
    "        self.uv365_backbone.fc = nn.Identity()\n",
    "        self.uv395_backbone.fc = nn.Identity()\n",
    "        \n",
    "        # 1x1 Conv layer to reduce concatenated features from 1536 -> 512\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=512*3, out_channels=512, kernel_size=1)\n",
    "        \n",
    "        # Classifier FC layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_white, x_uv365, x_uv395):\n",
    "        # Extract features (batch_size, 512)\n",
    "        f_white = self.white_backbone(x_white)\n",
    "        f_uv365 = self.uv365_backbone(x_uv365)\n",
    "        f_uv395 = self.uv395_backbone(x_uv395)\n",
    "        \n",
    "        # Concatenate features along channel dimension (batch_size, 1536)\n",
    "        fused = torch.cat([f_white, f_uv365, f_uv395], dim=1)\n",
    "        \n",
    "        # Reshape for conv layer: (batch_size, channels=1536, height=1, width=1)\n",
    "        fused = fused.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # Apply 1x1 convolution to reduce channels\n",
    "        fused = self.conv1x1(fused)  # (batch_size, 512, 1, 1)\n",
    "        \n",
    "        # Flatten back to (batch_size, 512)\n",
    "        fused = fused.view(fused.size(0), -1)\n",
    "        \n",
    "        # Pass through classifier\n",
    "        out = self.fc(fused)\n",
    "        \n",
    "        return out, fused  # Return prediction and embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbe1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match model input size\n",
    "    transforms.RandomHorizontalFlip(),  # Apply random horizontal flip (data augmentation)\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673cb12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/stud1/Desktop/PIL_MAIN/Leaf Dataset\"\n",
    "# Create dataset and split into train/val/test\n",
    "csv_file = os.path.join(root,\"labels.csv\")\n",
    "white_dir = os.path.join(root,\"white_Gamma\")\n",
    "uv365_dir = os.path.join(root,\"365UV\")\n",
    "uv395_dir = os.path.join(root,\"395UV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultimodalDataset(csv_file, white_dir, uv365_dir, uv395_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.05 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "seed =42\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size],generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalModelWithConvFusion().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ceb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop (Modified)\n",
    "def train_one_epoch_with_metrics(model, dataloader, criterion, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    for (white_imgs, uv365_imgs, uv395_imgs), labels in train_progress_bar:\n",
    "        white_imgs = white_imgs.to(device)\n",
    "        uv365_imgs = uv365_imgs.to(device)\n",
    "        uv395_imgs = uv395_imgs.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, _ = model(white_imgs, uv365_imgs, uv395_imgs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * white_imgs.size(0)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        correct += (preds == labels.byte()).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_with_metrics(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        for (white_imgs, uv365_imgs, uv395_imgs), labels in val_progress_bar:\n",
    "            white_imgs = white_imgs.to(device)\n",
    "            uv365_imgs = uv365_imgs.to(device)\n",
    "            uv395_imgs = uv395_imgs.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "            outputs, _ = model(white_imgs, uv365_imgs, uv395_imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * white_imgs.size(0)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            correct += (preds == labels.byte()).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Store predictions and labels for metrics\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76672533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (white_imgs, uv365_imgs, uv395_imgs), labels in tqdm(dataloader):\n",
    "            white_imgs = white_imgs.to(device)\n",
    "            uv365_imgs = uv365_imgs.to(device)\n",
    "            uv395_imgs = uv395_imgs.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "            outputs, _ = model(white_imgs, uv365_imgs, uv395_imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * white_imgs.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    \n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, cm, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')  # Start with infinity as the initial best loss\n",
    "best_model_path = \"./best_model.pth\"  # Path to save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf78142",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    train_loss, train_acc = train_one_epoch_with_metrics(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Validation Phase\n",
    "    val_loss, val_acc = validate_with_metrics(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # Check if the current validation loss is the best we've seen so far\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss  # Update the best validation loss\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save the model's state_dict\n",
    "\n",
    "    # Print metrics for this epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_prec, test_rec, f1, cm, roc_auc = test_model(model, test_loader, criterion, device)\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot Loss Curves\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
    "# plt.plot(range(1, num_epochs+1), val_losses, label=\"Validation Loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Epoch vs Loss\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot Accuracy Curves\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.plot(range(1, num_epochs+1), train_accuracies, label=\"Train Accuracy\")\n",
    "# plt.plot(range(1, num_epochs+1), val_accuracies, label=\"Validation Accuracy\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Epoch vs Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# Save Loss Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Epoch vs Loss for WhiteNoUV & 365NoUV & 395NoUV modalities\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"./loss_curve.png\")\n",
    "\n",
    "# Save Accuracy Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, num_epochs+1), val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Epoch vs Accuracy for WhiteNoUV & 365NoUV & 395NoUV modalities\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"./accuracy_curve.png\")\n",
    "\n",
    "class_names = [\"Healthy\", \"Unhealthy\"]  # Replace with your class names\n",
    "\n",
    "# Convert confusion matrix to a heatmap and save it as an image\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix \\n WhiteNoUV & 365NoUV & 395NoUV modalities\")\n",
    "plt.savefig(\"confusion_matrix.png\")  # Save the image\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
