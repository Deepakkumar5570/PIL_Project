{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.metrics import precision_recall_curve, accuracy_score, precision_score, recall_score, accuracy_score,f1_score, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a48ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, csv_file, white_Gamma_dir, Nouv365_dir, Nouv395_dir, white_dir, uv365_dir, uv395_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to CSV with columns: [filename, label]\n",
    "            white_Gamma_dir (str): Directory for white images with Gamma correction\n",
    "            Nouv365_dir (str): Directory for new 365nm UV images\n",
    "            Nouv395_dir (str): Directory for new 395nm UV images\n",
    "            white_dir (str): Directory for standard white images\n",
    "            uv365_dir (str): Directory for 365nm UV images\n",
    "            uv395_dir (str): Directory for 395nm UV images\n",
    "            transform (callable, optional): Transformations to apply to images\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.white_Gamma_dir = white_Gamma_dir\n",
    "        self.Nouv365_dir = Nouv365_dir\n",
    "        self.Nouv395_dir = Nouv395_dir\n",
    "        self.white_dir = white_dir\n",
    "        self.uv365_dir = uv365_dir\n",
    "        self.uv395_dir = uv395_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx]['filename']\n",
    "        img_name = str(int(img_name)) + '.jpg'\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # Load images from all six modalities\n",
    "        white_img = Image.open(os.path.join(self.white_dir, img_name)).convert('RGB')\n",
    "        uv365_img = Image.open(os.path.join(self.uv365_dir, img_name)).convert('RGB')\n",
    "        uv395_img = Image.open(os.path.join(self.uv395_dir, img_name)).convert('RGB')\n",
    "        white_gamma_img = Image.open(os.path.join(self.white_Gamma_dir, img_name)).convert('RGB')\n",
    "        nouv365_img = Image.open(os.path.join(self.Nouv365_dir, img_name)).convert('RGB')\n",
    "        nouv395_img = Image.open(os.path.join(self.Nouv395_dir, img_name)).convert('RGB')\n",
    "\n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            white_img = self.transform(white_img)\n",
    "            uv365_img = self.transform(uv365_img)\n",
    "            uv395_img = self.transform(uv395_img)\n",
    "            white_gamma_img = self.transform(white_gamma_img)\n",
    "            nouv365_img = self.transform(nouv365_img)\n",
    "            nouv395_img = self.transform(nouv395_img)\n",
    "\n",
    "        # Return a tuple of all six images and the label\n",
    "        images = (white_img, uv365_img, uv395_img, white_gamma_img, nouv365_img, nouv395_img)\n",
    "        return images, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e384373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultimodalModelWithConvFusion(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(MultimodalModelWithConvFusion, self).__init__()\n",
    "        \n",
    "        # Backbones for each modality (6 total)\n",
    "        self.white_backbone = models.resnet18(pretrained=True)\n",
    "        self.uv365_backbone = models.resnet18(pretrained=True)\n",
    "        self.uv395_backbone = models.resnet18(pretrained=True)\n",
    "        self.white_gamma_backbone = models.resnet18(pretrained=True)\n",
    "        self.nouv365_backbone = models.resnet18(pretrained=True)\n",
    "        self.nouv395_backbone = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Remove final fully connected layers for all backbones\n",
    "        self.white_backbone.fc = nn.Identity()\n",
    "        self.uv365_backbone.fc = nn.Identity()\n",
    "        self.uv395_backbone.fc = nn.Identity()\n",
    "        self.white_gamma_backbone.fc = nn.Identity()\n",
    "        self.nouv365_backbone.fc = nn.Identity()\n",
    "        self.nouv395_backbone.fc = nn.Identity()\n",
    "        \n",
    "        # 1x1 Conv to reduce concatenated features from 3072 -> 512\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=512 * 6, out_channels=512, kernel_size=1)\n",
    "        \n",
    "        # Classifier fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_white, x_uv365, x_uv395, x_white_gamma, x_nouv365, x_nouv395):\n",
    "        # Extract features from each backbone: (batch_size, 512)\n",
    "        f_white = self.white_backbone(x_white)\n",
    "        f_uv365 = self.uv365_backbone(x_uv365)\n",
    "        f_uv395 = self.uv395_backbone(x_uv395)\n",
    "        f_white_gamma = self.white_gamma_backbone(x_white_gamma)\n",
    "        f_nouv365 = self.nouv365_backbone(x_nouv365)\n",
    "        f_nouv395 = self.nouv395_backbone(x_nouv395)\n",
    "        \n",
    "        # Concatenate all features along channel dimension\n",
    "        fused = torch.cat([f_white, f_uv365, f_uv395, f_white_gamma, f_nouv365, f_nouv395], dim=1)\n",
    "        # fused shape: (batch_size, 3072)\n",
    "        \n",
    "        # Reshape for conv layer: (batch_size, 3072, 1, 1)\n",
    "        fused = fused.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # 1x1 convolution to reduce channels to 512\n",
    "        fused = self.conv1x1(fused)  # (batch_size, 512, 1, 1)\n",
    "        \n",
    "        # Flatten to (batch_size, 512)\n",
    "        fused = fused.view(fused.size(0), -1)\n",
    "        \n",
    "        # Classifier forward\n",
    "        out = self.fc(fused)\n",
    "        \n",
    "        return out, fused  # prediction and embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee4d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match model input size\n",
    "    transforms.RandomHorizontalFlip(),  # Apply random horizontal flip (data augmentation)\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99462fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/stud1/Desktop/PIL_MAIN/Leaf Dataset\"\n",
    "csv_file = os.path.join(root, \"labels.csv\")\n",
    "\n",
    "white_Gamma_dir = os.path.join(root, \"WhiteUV\")\n",
    "Nouv365_dir = os.path.join(root, \"365NoUV\")      # Assuming folder name \"Nouv365\"\n",
    "Nouv395_dir = os.path.join(root, \"395NoUV\")      # Assuming folder name \"Nouv395\"\n",
    "white_dir = os.path.join(root, \"WhiteNoUV\")          # Assuming folder name \"white\"\n",
    "uv365_dir = os.path.join(root, \"365UV\")\n",
    "uv395_dir = os.path.join(root, \"395UV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.70* total_size)\n",
    "val_size = int(0.10* total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "seed =42\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size],generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16,shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60596437",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalModelWithConvFusion().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72495199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def train_one_epoch_with_metrics(model, dataloader, criterion, optimizer, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    \n",
    "    for (white_imgs, uv365_imgs, uv395_imgs, white_gamma_imgs, nouv365_imgs, nouv395_imgs), labels in train_progress_bar:\n",
    "        # Move all inputs and labels to device\n",
    "        white_imgs = white_imgs.to(device)\n",
    "        uv365_imgs = uv365_imgs.to(device)\n",
    "        uv395_imgs = uv395_imgs.to(device)\n",
    "        white_gamma_imgs = white_gamma_imgs.to(device)\n",
    "        nouv365_imgs = nouv365_imgs.to(device)\n",
    "        nouv395_imgs = nouv395_imgs.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)  # Assuming binary classification\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with all 6 inputs\n",
    "        outputs, _ = model(white_imgs, uv365_imgs, uv395_imgs, white_gamma_imgs, nouv365_imgs, nouv395_imgs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * white_imgs.size(0)\n",
    "        \n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        correct += (preds == labels.byte()).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        train_progress_bar.set_postfix(loss=running_loss/total, accuracy=correct/total)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def validate_with_metrics(model, dataloader, criterion, device, epoch=None, num_epochs=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        desc = f\"Validation\"\n",
    "        if epoch is not None and num_epochs is not None:\n",
    "            desc = f\"Epoch {epoch+1}/{num_epochs} Validation\"\n",
    "\n",
    "        val_progress_bar = tqdm(dataloader, desc=desc, leave=False)\n",
    "        \n",
    "        for (white_imgs, uv365_imgs, uv395_imgs, white_gamma_imgs, nouv365_imgs, nouv395_imgs), labels in val_progress_bar:\n",
    "            # Move all inputs and labels to device\n",
    "            white_imgs = white_imgs.to(device)\n",
    "            uv365_imgs = uv365_imgs.to(device)\n",
    "            uv395_imgs = uv395_imgs.to(device)\n",
    "            white_gamma_imgs = white_gamma_imgs.to(device)\n",
    "            nouv365_imgs = nouv365_imgs.to(device)\n",
    "            nouv395_imgs = nouv395_imgs.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "            outputs, _ = model(white_imgs, uv365_imgs, uv395_imgs, white_gamma_imgs, nouv365_imgs, nouv395_imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * white_imgs.size(0)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            correct += (preds == labels.byte()).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            val_progress_bar.set_postfix(loss=running_loss/total, accuracy=correct/total)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "def test_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (white_imgs, uv365_imgs, uv395_imgs, white_gamma_imgs, nouv365_imgs, nouv395_imgs), labels in tqdm(dataloader):\n",
    "            # Move inputs and labels to device\n",
    "            white_imgs = white_imgs.to(device)\n",
    "            uv365_imgs = uv365_imgs.to(device)\n",
    "            uv395_imgs = uv395_imgs.to(device)\n",
    "            white_gamma_imgs = white_gamma_imgs.to(device)\n",
    "            nouv365_imgs = nouv365_imgs.to(device)\n",
    "            nouv395_imgs = nouv395_imgs.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "            outputs, _ = model(white_imgs, uv365_imgs, uv395_imgs, white_gamma_imgs, nouv365_imgs, nouv395_imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * white_imgs.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, cm, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbad684",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')  # Start with infinity as the initial best loss\n",
    "best_model_path = \"./best_model.pth\"  # Path to save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 11\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_model.pth\"\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Pass epoch and num_epochs here\n",
    "    train_loss, train_acc = train_one_epoch_with_metrics(model, train_loader, criterion, optimizer, device, epoch, num_epochs)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    val_loss, val_acc = validate_with_metrics(model, val_loader, criterion, device, epoch, num_epochs)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71359ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_prec, test_rec, f1, cm, roc_auc = test_model(model, test_loader, criterion, device)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d20cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot Loss Curves\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
    "# plt.plot(range(1, num_epochs+1), val_losses, label=\"Validation Loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Epoch vs Loss\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot Accuracy Curves\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.plot(range(1, num_epochs+1), train_accuracies, label=\"Train Accuracy\")\n",
    "# plt.plot(range(1, num_epochs+1), val_accuracies, label=\"Validation Accuracy\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Epoch vs Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# Save Loss Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Epoch vs Loss for all modalities\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"./loss_all_curve.png\")\n",
    "\n",
    "# Save Accuracy Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, num_epochs+1), val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Epoch vs Accuracy for all modalities\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"./accuracy_all_curve.png\")\n",
    "\n",
    "class_names = [\"Healthy\", \"Unhealthy\"]  # Replace with your class names\n",
    "\n",
    "# Convert confusion matrix to a heatmap and save it as an image\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix \\n all modalities\")\n",
    "plt.savefig(\"confusion_all_matrix.png\")  # Save the image\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
