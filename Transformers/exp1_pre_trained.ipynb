{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from helper_functions import set_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4801427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get pretrained weights for ViT-Base\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT \n",
    "\n",
    "# 2. Setup a ViT model instance with pretrained weights\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# 3. Freeze the base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "# 4. Change the classifier head \n",
    "class_names = ['daisy','dandelion']\n",
    "\n",
    "set_seeds()\n",
    "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "# pretrained_vit # uncomment for model output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit, \n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fcb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "train_dir = '/home/stud1/Desktop/PIL_MAIN/resnet!8/UV_RGB_T_T_V/test'\n",
    "test_dir = '/home/stud1/Desktop/PIL_MAIN/resnet!8/UV_RGB_T_T_V/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get automatic transforms from pretrained ViT weights\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "print(pretrained_vit_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloaders\n",
    "train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                     test_dir=test_dir,\n",
    "                                                                                                     transform=pretrained_vit_transforms,\n",
    "                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Create optimizer and loss function\n",
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n",
    "                             lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the classifier head of the pretrained ViT feature extractor model\n",
    "set_seeds()\n",
    "pretrained_vit_results = engine.train(model=pretrained_vit,\n",
    "                                      train_dataloader=train_dataloader_pretrained,\n",
    "                                      test_dataloader=test_dataloader_pretrained,\n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      epochs=15,\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1922c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(pretrained_vit_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472282c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Import function to make predictions on images and plot them \n",
    "from going_modular.going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "# Setup custom image path\n",
    "custom_image_path = \"/home/stud1/Desktop/PIL_MAIN/resnet!8/NoUV_T_T_V/test/1/365_25.jpg\"\n",
    "\n",
    "# Predict on custom image\n",
    "pred_and_plot_image(model=pretrained_vit,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Put model in evaluation mode\n",
    "pretrained_vit.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader_pretrained:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = pretrained_vit(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# Print test accuracy\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Directory to save results\n",
    "save_dir = \"/home/stud1/Desktop/PIL_MAIN/TransFormers/Image-Classification-Using-Vision-transformer/outputs\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# --- Confusion Matrix and Classification Report ---\n",
    "pretrained_vit.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader_pretrained:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = pretrained_vit(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('RGB_UV_Confusion Matrix')\n",
    "cm_path = os.path.join(save_dir, \"RGB_UV_confusion_matrix.png\")\n",
    "plt.savefig(cm_path)\n",
    "plt.close()\n",
    "print(f\"Confusion matrix saved to {cm_path}\")\n",
    "\n",
    "# Print classification report and test accuracy\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# --- Save Training/Validation Accuracy and Loss Curves ---\n",
    "def plot_and_save_loss_curves(results, save_path):\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, results[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs, results[\"test_loss\"], label=\"Val Loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, results[\"train_acc\"], label=\"Train Acc\")\n",
    "    plt.plot(epochs, results[\"test_acc\"], label=\"Val Acc\")\n",
    "    plt.title(\"NoUV_Accuracy Curves\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Training/validation curves saved to {save_path}\")\n",
    "\n",
    "plot_and_save_loss_curves(pretrained_vit_results, os.path.join(save_dir, \"RGB_UV_train_val_curves.png\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
