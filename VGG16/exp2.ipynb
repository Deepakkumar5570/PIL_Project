{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba59a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# 1a) Choose device: GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a69fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/stud1/Desktop/PIL_MAIN/Leaf Dataset\"\n",
    "csv_file = os.path.join(root, \"labels.csv\")\n",
    "\n",
    "# UV modality directories\n",
    "white_uv_dir = os.path.join(root, \"WhiteUV\")\n",
    "uv365_dir    = os.path.join(root, \"365UV\")\n",
    "uv395_dir    = os.path.join(root, \"395UV\")\n",
    "\n",
    "# NoUV (RGB) modality directories\n",
    "white_nouv_dir = os.path.join(root, \"WhiteNoUV\")\n",
    "nouv365_dir    = os.path.join(root, \"365NoUV\")\n",
    "nouv395_dir    = os.path.join(root, \"395NoUV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fedcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read CSV into a DataFrame\n",
    "# df_labels = pd.read_csv(csv_file)\n",
    "\n",
    "# # Create a dictionary: filename → label (int)\n",
    "# label_dict = dict(zip(df_labels[\"filename\"], df_labels[\"label\"]))\n",
    "\n",
    "\n",
    "\n",
    "# ...existing code...\n",
    "df_labels = pd.read_csv(csv_file)\n",
    "\n",
    "# Ensure filenames are strings and have .jpg extension\n",
    "df_labels[\"filename\"] = df_labels[\"filename\"].astype(str)\n",
    "if not df_labels[\"filename\"].iloc[0].endswith(\".jpg\"):\n",
    "    df_labels[\"filename\"] = df_labels[\"filename\"] + \".jpg\"\n",
    "\n",
    "label_dict = dict(zip(df_labels[\"filename\"], df_labels[\"label\"]))\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "\n",
    "class MultiModalLeafDataset(Dataset):\n",
    "    def __init__(self, filenames, labels, modality=\"uv\", img_size=(224, 224), augment=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filenames (list[str]): List of image filenames (e.g. [\"0.jpg\", \"1.jpg\", ...]).\n",
    "            labels (dict): Mapping from filename → integer label (0 or 1).\n",
    "            modality (str): \"uv\", \"rgb\", or \"uv_rgb\".\n",
    "            img_size (tuple[int,int]): (height, width) for resizing (default (224,224)).\n",
    "            augment (bool): If True, apply random flips/rotations; else no augmentation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.modality = modality.lower()\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "\n",
    "        # Define your directories for UV and RGB modalities (these must be set)\n",
    "        self.uv_dirs = [white_uv_dir, uv365_dir, uv395_dir]\n",
    "        self.rgb_dirs = [white_nouv_dir, nouv365_dir, nouv395_dir]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) Get filename and label\n",
    "        fname = self.filenames[idx]\n",
    "        label = self.labels[fname]  # 0 or 1\n",
    "\n",
    "        # Helper: load a single RGB image, resize, convert → tensor (shape [3, H, W])\n",
    "        def load_rgb_image(path):\n",
    "            img = Image.open(path).convert(\"RGB\")  # Convert to RGB\n",
    "            img = img.resize(self.img_size, resample=Image.BILINEAR)\n",
    "            tensor = TF.to_tensor(img)  # shape [3, H, W]\n",
    "            return tensor\n",
    "\n",
    "        # 2) For UV → load 3 RGB images and stack → [9, H, W]\n",
    "        if self.modality in (\"uv\", \"uv_rgb\"):\n",
    "            uv_tensors = []\n",
    "            for d in self.uv_dirs:\n",
    "                full_path = os.path.join(d, fname)\n",
    "                if not os.path.isfile(full_path):\n",
    "                    raise FileNotFoundError(f\"Expected UV file not found: {full_path}\")\n",
    "                uv_tensors.append(load_rgb_image(full_path))  # [3, H, W] each\n",
    "            uv_tensor = torch.cat(uv_tensors, dim=0)  # [9, H, W]\n",
    "\n",
    "        # 3) For RGB (NoUV) → load 3 RGB images and stack → [9, H, W]\n",
    "        if self.modality in (\"rgb\", \"uv_rgb\"):\n",
    "            rgb_tensors = []\n",
    "            for d in self.rgb_dirs:\n",
    "                full_path = os.path.join(d, fname)\n",
    "                if not os.path.isfile(full_path):\n",
    "                    raise FileNotFoundError(f\"Expected RGB file not found: {full_path}\")\n",
    "                rgb_tensors.append(load_rgb_image(full_path))  # [3, H, W] each\n",
    "            rgb_tensor = torch.cat(rgb_tensors, dim=0)  # [9, H, W]\n",
    "\n",
    "        # 4) Combine according to modality\n",
    "        if self.modality == \"uv\":\n",
    "            img_tensor = uv_tensor                 # [9, H, W]\n",
    "        elif self.modality == \"rgb\":\n",
    "            img_tensor = rgb_tensor                # [9, H, W]\n",
    "        elif self.modality == \"uv_rgb\":\n",
    "            img_tensor = torch.cat([uv_tensor, rgb_tensor], dim=0)  # [18, H, W]\n",
    "        else:\n",
    "            raise ValueError(f\"Modality must be 'uv', 'rgb', or 'uv_rgb', got '{self.modality}'\")\n",
    "\n",
    "        # 5) Apply identical augmentation (flips/rotations)\n",
    "        if self.augment:\n",
    "            # Random horizontal flip (50%)\n",
    "            if random.random() > 0.5:\n",
    "                img_tensor = torch.flip(img_tensor, dims=[2])  # flip width\n",
    "            # Random vertical flip (50%)\n",
    "            if random.random() > 0.5:\n",
    "                img_tensor = torch.flip(img_tensor, dims=[1])  # flip height\n",
    "            # Random rotation (0, 90, 180, 270 degrees)\n",
    "            angle = random.choice([0, 90, 180, 270])\n",
    "            if angle != 0:\n",
    "                img_tensor = TF.rotate(img_tensor, angle)\n",
    "\n",
    "        return img_tensor, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filenames = list(label_dict.keys())\n",
    "random.shuffle(all_filenames)\n",
    "\n",
    "n_total = len(all_filenames)\n",
    "n_train = int(0.70 * n_total)\n",
    "n_val   = int(0.10 * n_total)\n",
    "n_test  = n_total - n_train - n_val\n",
    "\n",
    "train_fnames = all_filenames[:n_train]\n",
    "val_fnames   = all_filenames[n_train : n_train + n_val]\n",
    "test_fnames  = all_filenames[n_train + n_val : ]\n",
    "\n",
    "print(f\"Total samples: {n_total}\")\n",
    "print(f\" → Train: {len(train_fnames)}, Val: {len(val_fnames)}, Test: {len(test_fnames)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a80384",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 4  # or 0 if working on Windows / Jupyter without multiprocessing\n",
    "\n",
    "# 6a) UV‐only\n",
    "train_uv_dataset = MultiModalLeafDataset(train_fnames, label_dict, modality=\"uv\",   img_size=(224,224), augment=True)\n",
    "val_uv_dataset   = MultiModalLeafDataset(val_fnames,   label_dict, modality=\"uv\",   img_size=(224,224), augment=False)\n",
    "test_uv_dataset  = MultiModalLeafDataset(test_fnames,  label_dict, modality=\"uv\",   img_size=(224,224), augment=False)\n",
    "\n",
    "train_uv_loader = DataLoader(train_uv_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
    "val_uv_loader   = DataLoader(val_uv_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_uv_loader  = DataLoader(test_uv_dataset,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# 6b) RGB‐only (NoUV)\n",
    "train_rgb_dataset = MultiModalLeafDataset(train_fnames, label_dict, modality=\"rgb\",  img_size=(224,224), augment=True)\n",
    "val_rgb_dataset   = MultiModalLeafDataset(val_fnames,   label_dict, modality=\"rgb\",  img_size=(224,224), augment=False)\n",
    "test_rgb_dataset  = MultiModalLeafDataset(test_fnames,  label_dict, modality=\"rgb\",  img_size=(224,224), augment=False)\n",
    "\n",
    "train_rgb_loader = DataLoader(train_rgb_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
    "val_rgb_loader   = DataLoader(val_rgb_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_rgb_loader  = DataLoader(test_rgb_dataset,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# 6c) UV+RGB early fusion\n",
    "train_uvrgb_dataset = MultiModalLeafDataset(train_fnames, label_dict, modality=\"uv_rgb\", img_size=(224,224), augment=True)\n",
    "val_uvrgb_dataset   = MultiModalLeafDataset(val_fnames,   label_dict, modality=\"uv_rgb\", img_size=(224,224), augment=False)\n",
    "test_uvrgb_dataset  = MultiModalLeafDataset(test_fnames,  label_dict, modality=\"uv_rgb\", img_size=(224,224), augment=False)\n",
    "\n",
    "train_uvrgb_loader = DataLoader(train_uvrgb_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
    "val_uvrgb_loader   = DataLoader(val_uvrgb_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_uvrgb_loader  = DataLoader(test_uvrgb_dataset,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05961103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vgg16_model(input_channels=3, num_classes=2, pretrained=True):\n",
    "    \"\"\"\n",
    "    Returns a VGG16-based model on `device`:\n",
    "      - If input_channels != 3, replaces first conv to accept `input_channels`.\n",
    "      - Adjusts final classifier to output `num_classes`.\n",
    "      - Uses pretrained ImageNet weights for everything else.\n",
    "    \"\"\"\n",
    "    # 1) Load standard pretrained VGG16\n",
    "    # model = models.vgg16(pretrained=pretrained)\n",
    "    model = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "    # ...existing code...\n",
    "\n",
    "    # 2) If we need a custom number of input channels, modify the first conv:\n",
    "    if input_channels != 3:\n",
    "        old_conv = model.features[0]  # original: Conv2d(3 → 64, kernel_size=3, padding=1)\n",
    "        new_conv = nn.Conv2d(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=old_conv.out_channels,\n",
    "            kernel_size=old_conv.kernel_size,\n",
    "            stride=old_conv.stride,\n",
    "            padding=old_conv.padding,\n",
    "            bias=(old_conv.bias is not None),\n",
    "        )\n",
    "        # Initialize new_conv weights by copying from old_conv\n",
    "        with torch.no_grad():\n",
    "            # Copy the first 3 channels from the pretrained weights\n",
    "            new_conv.weight[:, :3, :, :] = old_conv.weight\n",
    "            # For any extra channel (4..input_channels-1), we can copy the first channel’s weights:\n",
    "            for i in range(3, input_channels):\n",
    "                # Copy channel 0 of old_conv into channel i\n",
    "                new_conv.weight[:, i : i + 1, :, :] = old_conv.weight[:, :1, :, :]\n",
    "            # Copy bias if present\n",
    "            if old_conv.bias is not None:\n",
    "                new_conv.bias[:] = old_conv.bias[:]\n",
    "\n",
    "        # Replace the first conv layer in features\n",
    "        model.features[0] = new_conv\n",
    "\n",
    "    # 3) Replace the final classifier to output `num_classes` instead of 1000\n",
    "    #    VGG16’s default classifier[-1] is Linear(4096 → 1000). We need Linear(4096 → num_classes).\n",
    "    model.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)   # shape: [B, C, 224, 224]\n",
    "        labels = labels.to(device)   # shape: [B]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)      # shape: [B, 2]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc  = running_corrects.double() / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc  = running_corrects.double() / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a2e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modality, train_loader, val_loader, num_epochs=20, lr=1e-4, patience=5):\n",
    "    \"\"\"\n",
    "    Trains a VGG16-based model for the given modality with early stopping.\n",
    "    Returns: (best_model, best_validation_accuracy)\n",
    "    \"\"\"\n",
    "    if modality in (\"uv\", \"rgb\"):\n",
    "        in_channels = 9\n",
    "    elif modality == \"uv_rgb\":\n",
    "        in_channels = 18\n",
    "    else:\n",
    "        raise ValueError(\"Modality must be 'uv', 'rgb', or 'uv_rgb'.\")\n",
    "\n",
    "    model = create_vgg16_model(input_channels=in_channels, num_classes=2, pretrained=True)\n",
    "    # Unfreeze last two convolutional blocks for fine-tuning\n",
    "    for param in model.features[24:].parameters():\n",
    "       param.requires_grad = True\n",
    "       \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc     = validate_one_epoch(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"[{modality.upper()}] Epoch {epoch+1}/{num_epochs}  \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}  \"\n",
    "              f\"|  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        # Save best validation weights\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f765a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10a) UV‐only\n",
    "print(\"→ Training UV‐only model …\")\n",
    "uv_model, uv_best_acc = train_model(\n",
    "    modality=\"uv\",\n",
    "    train_loader=train_uv_loader,\n",
    "    val_loader=val_uv_loader,\n",
    "    num_epochs=20,\n",
    "    lr=1e-4\n",
    ")\n",
    "print(f\"★ Best UV Validation Accuracy: {uv_best_acc:.4f}\\n\")\n",
    "\n",
    "# 10b) RGB‐only\n",
    "print(\"→ Training RGB‐only model …\")\n",
    "rgb_model, rgb_best_acc = train_model(\n",
    "    modality=\"rgb\",\n",
    "    train_loader=train_rgb_loader,\n",
    "    val_loader=val_rgb_loader,\n",
    "    num_epochs=20,\n",
    "    lr=1e-4\n",
    ")\n",
    "print(f\"★ Best RGB Validation Accuracy: {rgb_best_acc:.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 10c) UV+RGB (6-channel early fusion)\n",
    "print(\"→ Training UV+RGB model …\")\n",
    "uvrgb_model, uvrgb_best_acc = train_model(\n",
    "    modality=\"uv_rgb\",\n",
    "    train_loader=train_uvrgb_loader,\n",
    "    val_loader=val_uvrgb_loader,\n",
    "    num_epochs=20,\n",
    "    lr=1e-4\n",
    ")\n",
    "print(f\"★ Best UV+RGB Validation Accuracy: {uvrgb_best_acc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee74589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    test_acc = running_corrects.double() / len(test_loader.dataset)\n",
    "    return test_acc.item()\n",
    "\n",
    "uv_test_acc    = test_model(uv_model, test_uv_loader)\n",
    "rgb_test_acc   = test_model(rgb_model, test_rgb_loader)\n",
    "# uvrgb_test_acc = test_model(uvrgb_model, test_uvrgb_loader)\n",
    "\n",
    "print(f\"→ UV Test Accuracy:    {uv_test_acc:.4f}\")\n",
    "print(f\"→ RGB Test Accuracy:   {rgb_test_acc:.4f}\")\n",
    "# print(f\"→ UV+RGB Test Accuracy: {uvrgb_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def eval_metrics(model, test_loader, name=\"Model\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"\\n{name} Precision: {precision:.4f}\")\n",
    "    print(f\"{name} Recall:    {recall:.4f}\")\n",
    "    print(f\"{name} Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "eval_metrics(uv_model, test_uv_loader, name=\"UV\")\n",
    "eval_metrics(rgb_model, test_rgb_loader, name=\"RGB\")\n",
    "eval_metrics(uvrgb_model, test_uvrgb_loader, name=\"UV+RGB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
