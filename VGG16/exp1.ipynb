{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe095e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# 1a) Choose device: GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/stud1/Desktop/PIL_MAIN/Leaf Dataset\"\n",
    "csv_file = os.path.join(root, \"labels.csv\")\n",
    "\n",
    "# UV modality directories\n",
    "white_uv_dir = os.path.join(root, \"WhiteUV\")\n",
    "uv365_dir    = os.path.join(root, \"365UV\")\n",
    "uv395_dir    = os.path.join(root, \"395UV\")\n",
    "\n",
    "# NoUV (RGB) modality directories\n",
    "white_nouv_dir = os.path.join(root, \"WhiteNoUV\")\n",
    "nouv365_dir    = os.path.join(root, \"365NoUV\")\n",
    "nouv395_dir    = os.path.join(root, \"395NoUV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71269e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read CSV into a DataFrame\n",
    "# df_labels = pd.read_csv(csv_file)\n",
    "\n",
    "# # Create a dictionary: filename → label (int)\n",
    "# label_dict = dict(zip(df_labels[\"filename\"], df_labels[\"label\"]))\n",
    "\n",
    "\n",
    "\n",
    "# ...existing code...\n",
    "df_labels = pd.read_csv(csv_file)\n",
    "\n",
    "# Ensure filenames are strings and have .jpg extension\n",
    "df_labels[\"filename\"] = df_labels[\"filename\"].astype(str)\n",
    "if not df_labels[\"filename\"].iloc[0].endswith(\".jpg\"):\n",
    "    df_labels[\"filename\"] = df_labels[\"filename\"] + \".jpg\"\n",
    "\n",
    "label_dict = dict(zip(df_labels[\"filename\"], df_labels[\"label\"]))\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b88ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalLeafDataset(Dataset):\n",
    "    def __init__(self, filenames, labels, modality=\"uv\", img_size=(224, 224), augment=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filenames (list[str]): List of image filenames (e.g. [\"0.jpg\", \"1.jpg\", ...]).\n",
    "            labels (dict): Mapping from filename → integer label (0 or 1).\n",
    "            modality (str): \"uv\", \"rgb\", or \"uv_rgb\".\n",
    "            img_size (tuple[int,int]): (height, width) for resizing (default (224,224)).\n",
    "            augment (bool): If True, apply random flips/rotations; else no augmentation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.modality = modality.lower()\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "\n",
    "        # UV subfolders (3 grayscale maps)\n",
    "        self.uv_dirs = [white_uv_dir, uv365_dir, uv395_dir]\n",
    "        # RGB/NoUV subfolders (3 grayscale maps)\n",
    "        self.rgb_dirs = [white_nouv_dir, nouv365_dir, nouv395_dir]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) Get filename and label\n",
    "        fname = self.filenames[idx]\n",
    "        label = self.labels[fname]  # 0 or 1\n",
    "\n",
    "        # Helper: load a single grayscale image, resize, convert → tensor (shape [1, H, W])\n",
    "        def load_single_channel(path):\n",
    "            img = Image.open(path).convert(\"L\")                        # \"L\" = grayscale\n",
    "            img = img.resize(self.img_size, resample=Image.BILINEAR)   # resize to (224×224)\n",
    "            tensor = TF.to_tensor(img)  # yields a torch.FloatTensor of shape [1, H, W], values in [0,1]\n",
    "            return tensor\n",
    "\n",
    "        # 2) For UV → load 3 grayscale images and stack → [3, H, W]\n",
    "        if self.modality in (\"uv\", \"uv_rgb\"):\n",
    "            uv_tensors = []\n",
    "            for d in self.uv_dirs:\n",
    "                full_path = os.path.join(d, fname)\n",
    "                if not os.path.isfile(full_path):\n",
    "                    raise FileNotFoundError(f\"Expected UV file not found: {full_path}\")\n",
    "                uv_tensors.append(load_single_channel(full_path))\n",
    "            uv_tensor = torch.cat(uv_tensors, dim=0)  # shape [3, 224, 224]\n",
    "\n",
    "        # 3) For RGB (NoUV) → load 3 grayscale images and stack → [3, H, W]\n",
    "        if self.modality in (\"rgb\", \"uv_rgb\"):\n",
    "            rgb_tensors = []\n",
    "            for d in self.rgb_dirs:\n",
    "                full_path = os.path.join(d, fname)\n",
    "                if not os.path.isfile(full_path):\n",
    "                    raise FileNotFoundError(f\"Expected RGB file not found: {full_path}\")\n",
    "                rgb_tensors.append(load_single_channel(full_path))\n",
    "            rgb_tensor = torch.cat(rgb_tensors, dim=0)  # shape [3, 224, 224]\n",
    "\n",
    "        # 4) Combine according to modality\n",
    "        if self.modality == \"uv\":\n",
    "            img_tensor = uv_tensor                 # [3, 224, 224]\n",
    "        elif self.modality == \"rgb\":\n",
    "            img_tensor = rgb_tensor                # [3, 224, 224]\n",
    "        elif self.modality == \"uv_rgb\":\n",
    "            img_tensor = torch.cat([uv_tensor, rgb_tensor], dim=0)  # [6, 224, 224]\n",
    "        else:\n",
    "            raise ValueError(f\"Modality must be 'uv', 'rgb', or 'uv_rgb', got '{self.modality}'\")\n",
    "\n",
    "        # 5) If augmentation is ON, apply identical random flips/rotations to all channels\n",
    "        if self.augment:\n",
    "            # Random horizontal flip (50% chance)\n",
    "            if random.random() > 0.5:\n",
    "                img_tensor = torch.flip(img_tensor, dims=[2])  # flip width\n",
    "\n",
    "            # Random vertical flip (50% chance)\n",
    "            if random.random() > 0.5:\n",
    "                img_tensor = torch.flip(img_tensor, dims=[1])  # flip height\n",
    "\n",
    "            # Random 0°/90°/180°/270° rotation\n",
    "            angle = random.choice([0, 90, 180, 270])\n",
    "            if angle != 0:\n",
    "                img_tensor = TF.rotate(img_tensor, angle)\n",
    "\n",
    "        return img_tensor, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fe8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filenames = list(label_dict.keys())\n",
    "random.shuffle(all_filenames)\n",
    "\n",
    "n_total = len(all_filenames)\n",
    "n_train = int(0.70 * n_total)\n",
    "n_val   = int(0.15 * n_total)\n",
    "n_test  = n_total - n_train - n_val\n",
    "\n",
    "train_fnames = all_filenames[:n_train]\n",
    "val_fnames   = all_filenames[n_train : n_train + n_val]\n",
    "test_fnames  = all_filenames[n_train + n_val : ]\n",
    "\n",
    "print(f\"Total samples: {n_total}\")\n",
    "print(f\" → Train: {len(train_fnames)}, Val: {len(val_fnames)}, Test: {len(test_fnames)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 4  # or 0 if working on Windows / Jupyter without multiprocessing\n",
    "\n",
    "# 6a) UV‐only\n",
    "train_uv_dataset = MultiModalLeafDataset(train_fnames, label_dict, modality=\"uv\",   img_size=(224,224), augment=True)\n",
    "val_uv_dataset   = MultiModalLeafDataset(val_fnames,   label_dict, modality=\"uv\",   img_size=(224,224), augment=False)\n",
    "test_uv_dataset  = MultiModalLeafDataset(test_fnames,  label_dict, modality=\"uv\",   img_size=(224,224), augment=False)\n",
    "\n",
    "train_uv_loader = DataLoader(train_uv_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
    "val_uv_loader   = DataLoader(val_uv_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_uv_loader  = DataLoader(test_uv_dataset,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# 6b) RGB‐only (NoUV)\n",
    "train_rgb_dataset = MultiModalLeafDataset(train_fnames, label_dict, modality=\"rgb\",  img_size=(224,224), augment=True)\n",
    "val_rgb_dataset   = MultiModalLeafDataset(val_fnames,   label_dict, modality=\"rgb\",  img_size=(224,224), augment=False)\n",
    "test_rgb_dataset  = MultiModalLeafDataset(test_fnames,  label_dict, modality=\"rgb\",  img_size=(224,224), augment=False)\n",
    "\n",
    "train_rgb_loader = DataLoader(train_rgb_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
    "val_rgb_loader   = DataLoader(val_rgb_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_rgb_loader  = DataLoader(test_rgb_dataset,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# 6c) UV+RGB early fusion\n",
    "train_uvrgb_dataset = MultiModalLeafDataset(train_fnames, label_dict, modality=\"uv_rgb\", img_size=(224,224), augment=True)\n",
    "val_uvrgb_dataset   = MultiModalLeafDataset(val_fnames,   label_dict, modality=\"uv_rgb\", img_size=(224,224), augment=False)\n",
    "test_uvrgb_dataset  = MultiModalLeafDataset(test_fnames,  label_dict, modality=\"uv_rgb\", img_size=(224,224), augment=False)\n",
    "\n",
    "train_uvrgb_loader = DataLoader(train_uvrgb_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
    "val_uvrgb_loader   = DataLoader(val_uvrgb_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_uvrgb_loader  = DataLoader(test_uvrgb_dataset,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afe3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vgg16_model(input_channels=3, num_classes=2, pretrained=True):\n",
    "    \"\"\"\n",
    "    Returns a VGG16-based model on `device`:\n",
    "      - If input_channels != 3, replaces first conv to accept `input_channels`.\n",
    "      - Adjusts final classifier to output `num_classes`.\n",
    "      - Uses pretrained ImageNet weights for everything else.\n",
    "    \"\"\"\n",
    "    # 1) Load standard pretrained VGG16\n",
    "    model = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "    # 2) If we need a custom number of input channels, modify the first conv:\n",
    "    if input_channels != 3:\n",
    "        old_conv = model.features[0]  # original: Conv2d(3 → 64, kernel_size=3, padding=1)\n",
    "        new_conv = nn.Conv2d(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=old_conv.out_channels,\n",
    "            kernel_size=old_conv.kernel_size,\n",
    "            stride=old_conv.stride,\n",
    "            padding=old_conv.padding,\n",
    "            bias=(old_conv.bias is not None),\n",
    "        )\n",
    "        # Initialize new_conv weights by copying from old_conv\n",
    "        with torch.no_grad():\n",
    "            # Copy the first 3 channels from the pretrained weights\n",
    "            new_conv.weight[:, :3, :, :] = old_conv.weight\n",
    "            # For any extra channel (4..input_channels-1), we can copy the first channel’s weights:\n",
    "            for i in range(3, input_channels):\n",
    "                # Copy channel 0 of old_conv into channel i\n",
    "                new_conv.weight[:, i : i + 1, :, :] = old_conv.weight[:, :1, :, :]\n",
    "            # Copy bias if present\n",
    "            if old_conv.bias is not None:\n",
    "                new_conv.bias[:] = old_conv.bias[:]\n",
    "\n",
    "        # Replace the first conv layer in features\n",
    "        model.features[0] = new_conv\n",
    "\n",
    "    # 3) Replace the final classifier to output `num_classes` instead of 1000\n",
    "    #    VGG16’s default classifier[-1] is Linear(4096 → 1000). We need Linear(4096 → num_classes).\n",
    "    model.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1151f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)   # shape: [B, C, 224, 224]\n",
    "        labels = labels.to(device)   # shape: [B]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)      # shape: [B, 2]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc  = running_corrects.double() / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc  = running_corrects.double() / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modality, train_loader, val_loader, num_epochs=20, lr=1e-4, patience=5):\n",
    "    \"\"\"\n",
    "    Trains a VGG16-based model for the given modality with early stopping.\n",
    "    Returns: (best_model, best_validation_accuracy)\n",
    "    \"\"\"\n",
    "    if modality in (\"uv\", \"rgb\"):\n",
    "        in_channels = 3\n",
    "    elif modality == \"uv_rgb\":\n",
    "        in_channels = 6\n",
    "    else:\n",
    "        raise ValueError(\"Modality must be 'uv', 'rgb', or 'uv_rgb'.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # model = create_vgg16_model(input_channels=in_channels, num_classes=2, pretrained=True)\n",
    "    model = create_vgg16_model(input_channels=in_channels, num_classes=2, pretrained=True)\n",
    "    # Freeze all feature layers (convolutional layers)\n",
    "    for param in model.features.parameters():\n",
    "       param.requires_grad = False\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc     = validate_one_epoch(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"[{modality.upper()}] Epoch {epoch+1}/{num_epochs}  \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}  \"\n",
    "              f\"|  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modality, train_loader, val_loader, num_epochs=20, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Trains a VGG16-based model for the given modality.\n",
    "    modality: \"uv\" → 3-channel UV, \n",
    "              \"rgb\" → 3-channel NoUV (RGB), \n",
    "              \"uv_rgb\" → 6-channel early fusion.\n",
    "    Returns: (best_model, best_validation_accuracy)\n",
    "    \"\"\"\n",
    "    # Determine number of input channels\n",
    "    if modality in (\"uv\", \"rgb\"):\n",
    "        in_channels = 3\n",
    "    elif modality == \"uv_rgb\":\n",
    "        in_channels = 6\n",
    "    else:\n",
    "        raise ValueError(\"Modality must be 'uv', 'rgb', or 'uv_rgb'.\")\n",
    "\n",
    "    # 1) Create model\n",
    "    model = create_vgg16_model(input_channels=in_channels, num_classes=2, pretrained=True)\n",
    "\n",
    "    # 2) Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc     = validate_one_epoch(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"[{modality.upper()}] Epoch {epoch+1}/{num_epochs}  \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}  \"\n",
    "              f\"|  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save best validation weights\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modality, train_loader, val_loader, num_epochs=20, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Trains a VGG16-based model for the given modality.\n",
    "    modality: \"uv\" → 3-channel UV, \n",
    "              \"rgb\" → 3-channel NoUV (RGB), \n",
    "              \"uv_rgb\" → 6-channel early fusion.\n",
    "    Returns: (best_model, best_validation_accuracy)\n",
    "    \"\"\"\n",
    "    # Determine number of input channels\n",
    "    if modality in (\"uv\", \"rgb\"):\n",
    "        in_channels = 3\n",
    "    elif modality == \"uv_rgb\":\n",
    "        in_channels = 6\n",
    "    else:\n",
    "        raise ValueError(\"Modality must be 'uv', 'rgb', or 'uv_rgb'.\")\n",
    "\n",
    "    # 1) Create model\n",
    "    model = create_vgg16_model(input_channels=in_channels, num_classes=2, pretrained=True)\n",
    "\n",
    "    # 2) Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc     = validate_one_epoch(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"[{modality.upper()}] Epoch {epoch+1}/{num_epochs}  \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}  \"\n",
    "              f\"|  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save best validation weights\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10a) UV‐only\n",
    "print(\"→ Training UV‐only model …\")\n",
    "uv_model, uv_best_acc = train_model(\n",
    "    modality=\"uv\",\n",
    "    train_loader=train_uv_loader,\n",
    "    val_loader=val_uv_loader,\n",
    "    num_epochs=20,\n",
    "    lr=1e-4\n",
    ")\n",
    "print(f\"★ Best UV Validation Accuracy: {uv_best_acc:.4f}\\n\")\n",
    "\n",
    "# 10b) RGB‐only\n",
    "print(\"→ Training RGB‐only model …\")\n",
    "rgb_model, rgb_best_acc = train_model(\n",
    "    modality=\"rgb\",\n",
    "    train_loader=train_rgb_loader,\n",
    "    val_loader=val_rgb_loader,\n",
    "    num_epochs=20,\n",
    "    lr=1e-4\n",
    ")\n",
    "print(f\"★ Best RGB Validation Accuracy: {rgb_best_acc:.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# 10c) UV+RGB (6-channel early fusion)\n",
    "print(\"→ Training UV+RGB model …\")\n",
    "uvrgb_model, uvrgb_best_acc = train_model(\n",
    "    modality=\"uv_rgb\",\n",
    "    train_loader=train_uvrgb_loader,\n",
    "    val_loader=val_uvrgb_loader,\n",
    "    num_epochs=20,\n",
    "    lr=1e-4\n",
    ")\n",
    "print(f\"★ Best UV+RGB Validation Accuracy: {uvrgb_best_acc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28885753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    test_acc = running_corrects.double() / len(test_loader.dataset)\n",
    "    return test_acc.item()\n",
    "\n",
    "uv_test_acc    = test_model(uv_model, test_uv_loader)\n",
    "rgb_test_acc   = test_model(rgb_model, test_rgb_loader)\n",
    "uvrgb_test_acc = test_model(uvrgb_model, test_uvrgb_loader)\n",
    "\n",
    "print(f\"→ UV Test Accuracy:    {uv_test_acc:.4f}\")\n",
    "print(f\"→ RGB Test Accuracy:   {rgb_test_acc:.4f}\")\n",
    "print(f\"→ UV+RGB Test Accuracy: {uvrgb_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def eval_metrics(model, test_loader, name=\"Model\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"\\n{name} Precision: {precision:.4f}\")\n",
    "    print(f\"{name} Recall:    {recall:.4f}\")\n",
    "    print(f\"{name} Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "eval_metrics(uv_model, test_uv_loader, name=\"UV\")\n",
    "eval_metrics(rgb_model, test_rgb_loader, name=\"RGB\")\n",
    "eval_metrics(uvrgb_model, test_uvrgb_loader, name=\"UV+RGB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
